{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b95d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "AVAIL_GPU = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS = int(os.cpu_count() / 2) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4265c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir = \"./data\",\n",
    "                 batch_size = BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.1307), (0.3081))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fea8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_obj = MNISTDataModule()\n",
    "mnist_obj.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da21711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detective: fake or no fake -> 1 output [0, 1]\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Simple CNN\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv1 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        # Flatten the tensor so it can be fed into the FC layer\n",
    "        x = x.view(-1,320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad624458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(latent_dim, 7*7*64)  # [n, 256, 7, 7]\n",
    "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2) # [n, 64, 16, 16]\n",
    "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2) # [n, 16, 34, 34]\n",
    "        self.conv = nn.Conv2d(16, 1, kernel_size=7) # [n, 16, 34, 34]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass latent space input into linear layer and reshape\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 64, 7, 7) #256\n",
    "\n",
    "        # Upsample (transposed conv) 16x16 (64 feature maps)\n",
    "        x = self.ct1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Upsample to 34x34 (16 feature maps)\n",
    "        x = self.ct2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Convolution to 28x28 (1 feature map)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf3445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=100, lr=0.0002):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        print(self.hparams)\n",
    "        self.generator = Generator(self.hparams.latent_dim) # type: ignore\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "        # random noise\n",
    "        self.validation_z = torch.rand(6, self.hparams.latent_dim) # type: ignore\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat,y)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_imgs, _ = batch\n",
    "\n",
    "        # smaple noise\n",
    "        z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim) # type: ignore\n",
    "        z = z.type_as(real_imgs)\n",
    "\n",
    "        # train generator: max log(D(G(z)))\n",
    "        if optimizer_idx == 0:\n",
    "            fake_imgs = self(z)\n",
    "            y_hat = self.discriminator(fake_imgs)\n",
    "            y = torch.ones(real_imgs.size(0), 1)\n",
    "            y = y.type_as(real_imgs)\n",
    "\n",
    "            g_loss = self.adversarial_loss(y_hat, y)\n",
    "\n",
    "            log_dict = {\"g_loss\": g_loss}\n",
    "            return {\n",
    "                \"loss\": g_loss,\n",
    "                \"progress_bar\": log_dict,\n",
    "                \"log\": log_dict\n",
    "            }\n",
    "        \n",
    "        # train discriminator: max log(D(x)) + log(1-D(G(z)))\n",
    "        if optimizer_idx == 1:\n",
    "\n",
    "            # how well can it label as real\n",
    "            y_hat_real = self.discriminator(real_imgs)\n",
    "            y_real = torch.ones(real_imgs.size(0), 1)\n",
    "            y_real = y_real.type_as(real_imgs)\n",
    "\n",
    "            real_loss = self.adversarial_loss(y_hat_real, y_real)\n",
    "\n",
    "            # how well can it label as fake\n",
    "            y_hat_fake = self.discriminator(self(z).detach())\n",
    "            y_fake = torch.zeros(real_imgs.size(0), 1)\n",
    "            y_fake = y_fake.type_as(real_imgs)\n",
    "\n",
    "            fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            log_dict = {\"d_loss\": d_loss}\n",
    "            return {\"loss\": d_loss, \"progress_bar\": log_dict, \"log\": log_dict}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr # type: ignore\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "    def plot_imgs(self):\n",
    "        z = self.validation_z.type_as(self.generator.lin1.weight)\n",
    "        sample_imgs = self(z).cpu()\n",
    "        print(\"EPOCH:\", self.current_epoch)\n",
    "        fig = plt.figure()\n",
    "        for i in range(sample_imgs.size(0)):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.tight_layout()\n",
    "            plt.imshow(sample_imgs.detach()[i,0,:,:], cmap='gray_r', interpolation='none')\n",
    "            plt.title(\"Generated Data\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.plot_imgs()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272a25e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"latent_dim\": 100\n",
      "\"lr\":         0.0002\n"
     ]
    }
   ],
   "source": [
    "dm = MNISTDataModule()\n",
    "model = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502a2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx` argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers in `training_step` with `opt1, opt2, ... = self.optimizers()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:958\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m    957\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:159\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:139\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:194\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    188\u001b[0m optimizers, lr_schedulers, monitor \u001b[38;5;241m=\u001b[39m _configure_optimizers(optim_conf)\n\u001b[0;32m    189\u001b[0m lr_scheduler_configs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    190\u001b[0m     _configure_schedulers_automatic_opt(lr_schedulers, monitor)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _configure_schedulers_manual_opt(lr_schedulers)\n\u001b[0;32m    193\u001b[0m )\n\u001b[1;32m--> 194\u001b[0m \u001b[43m_validate_multiple_optimizers_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m _validate_optimizers_attached(optimizers, lr_scheduler_configs)\n\u001b[0;32m    196\u001b[0m _validate_scheduler_api(lr_scheduler_configs, model)\n",
      "File \u001b[1;32mc:\\Users\\yashgupta\\anaconda3\\envs\\myenv39\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:353\u001b[0m, in \u001b[0;36m_validate_multiple_optimizers_support\u001b[1;34m(optimizers, model)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_multiple_optimizers_support\u001b[39m(optimizers: \u001b[38;5;28mlist\u001b[39m[Optimizer], model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_param_in_hook_signature(model\u001b[38;5;241m.\u001b[39mtraining_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `training_step` with `opt1, opt2, ... = self.optimizers()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         )\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with multiple optimizers is only supported with manual optimization. Set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `self.automatic_optimization = False`, then access your optimizers in `training_step` with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `opt1, opt2, ... = self.optimizers()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Training with multiple optimizers is only supported with manual optimization. Remove the `optimizer_idx` argument from `training_step`, set `self.automatic_optimization = False` and access your optimizers in `training_step` with `opt1, opt2, ... = self.optimizers()`."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model,dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed574e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3e5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
